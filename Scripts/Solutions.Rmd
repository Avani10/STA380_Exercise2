---
title: "Exercise2"
date: "18 August 2018"
output: 
  github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```







##Problem1

Loading and cleaning data
```{r}
ABIA=read.csv("../data/ABIA.csv")
ABIA$ArrDelay[is.na(ABIA$ArrDelay)] = 0
codes=read.csv("../data/airport-codes.csv")
codes1=codes[c('name','coordinates','iata_code')]
codes1=na.omit(codes1)
codes1=codes1[!duplicated(codes1[c('iata_code')]),]
ABIA$Week<- ABIA$DayOfWeek
ABIA$Week[ABIA$DayOfWeek == 1] <- 'Monday'
ABIA$Week[ABIA$DayOfWeek == 2] <- 'Tuesday'
ABIA$Week[ABIA$DayOfWeek == 3] <- 'Wednesday'
ABIA$Week[ABIA$DayOfWeek == 4] <- 'Thursday'
ABIA$Week[ABIA$DayOfWeek == 5] <- 'Friday'
ABIA$Week[ABIA$DayOfWeek == 6] <- 'Saturday'
ABIA$Week[ABIA$DayOfWeek == 7] <- 'Sunday'
ABIA$MonthQual<-ABIA$Month
ABIA$MonthQual[ABIA$MonthQual == 1] <- 'January'
ABIA$MonthQual[ABIA$MonthQual == 2] <- 'February'
ABIA$MonthQual[ABIA$MonthQual == 3] <- 'March'
ABIA$MonthQual[ABIA$MonthQual == 4] <- 'April'
ABIA$MonthQual[ABIA$MonthQual == 5] <- 'May'
ABIA$MonthQual[ABIA$MonthQual == 6] <- 'June'
ABIA$MonthQual[ABIA$MonthQual == 7] <-'July'
ABIA$MonthQual[ABIA$MonthQual == 8] <- 'August'
ABIA$MonthQual[ABIA$MonthQual == 9] <- 'September'
ABIA$MonthQual[ABIA$MonthQual == 10] <- 'October'
ABIA$MonthQual[ABIA$MonthQual == 11] <- 'November'
ABIA$MonthQual[ABIA$MonthQual == 12] <- 'December'

ABIA$CancellationCode1="No cancellation"
ABIA$CancellationCode1[ABIA$CancellationCode =='A']="Carrier"
ABIA$CancellationCode1[ABIA$CancellationCode == 'B'] = 'Weather'
ABIA$CancellationCode1[ABIA$CancellationCode == 'C'] = 'NAS'
ABIA$CancellationCode1[ABIA$CancellationCode == 'D'] = 'Security'

ABIA=merge(ABIA, codes1, by.x = "Dest", by.y = "iata_code")

ABIA$Dephr=substr(ABIA$DepTime,1,nchar(ABIA$DepTime)-2)
ABIA$Dephr[ABIA$Dephr == ''] = 0
ABIA$Dephr=as.numeric(ABIA$Dephr)

ABIA$Direction[ABIA$Dest=="AUS"]<-"To Aus"
ABIA$Direction[ABIA$Origin=="AUS"]<-"From Aus"
ABIA_notcancelled=ABIA[ABIA$Cancelled==0,]
```

```{r }

library(ggplot2)
ggplot(ABIA_notcancelled, aes(Dephr)) + geom_bar(width=0.8,position="dodge",color="black")+
  labs(x = "Dephr", y = "Count",title = "#Flights to and from Austin")+
  theme(legend.text = element_text(colour="blue", size=10, 
                                   face="bold"))

```

```{r }
FlighttofromAUS=table(ABIA_notcancelled$Direction,ABIA_notcancelled$Dephr)
barplot(FlighttofromAUS, main="Successful flights",
        xlab="Dep hr", col=c("orange","blue"),beside = TRUE,legend = rownames(FlighttofromAUS))
```

More traffic from 6AM-7PM

```{r}
DF_Q1=cbind(aggregate(ArrDelay ~ Dephr+DayOfWeek, data = ABIA_notcancelled, FUN = function(x) c(mn = mean(x))),aggregate(ArrDelay ~ Dephr+Week, data = ABIA_notcancelled, FUN = function(x) c(mn = length(x))))
DF_Q1=DF_Q1[,c(1,2,3,6)]
colnames(DF_Q1)[3] <- "Avg_Arr_Delay"
colnames(DF_Q1)[4] <- "flights"
DF_Q1=na.omit(DF_Q1)
```

```{r,warning=FALSE}
library(ggplot2)
require(viridis)
library(ggthemes)

ggplot(data = DF_Q1, aes(x=Dephr,y=as.factor(DayOfWeek))) + geom_tile(aes(fill=flights)) + scale_fill_viridis(option="magma")+xlab('Dephr')+theme_tufte(base_family="Helvetica")+ggtitle("No of Flights")+xlab("Time")

```

```{r,warning=FALSE}
ggplot(data = DF_Q1, aes(x = Dephr ,y = as.factor(DayOfWeek))) + 
  geom_tile(aes(fill = Avg_Arr_Delay)) + 
  coord_equal(ratio = 1)+ scale_fill_viridis(option="magma") + theme_tufte(base_family="Helvetica")+ggtitle("Avg delay in minutes")+xlab("Departure time")

```

Though there are very few flights during early mornings, avg delays tend to be longer for these flights 

```{r,warning=FALSE}
ABIA$CSRDephr_Correct = substr(ABIA$CRSDepTime,1,nchar(ABIA$CRSDepTime)-2)
ABIA$CSRDephr_Correct=as.numeric(ABIA$CSRDephr_Correct)
ABIA_cancelled=ABIA[ABIA$Cancelled==1,]
DF_Q1_1=aggregate(Year ~ CSRDephr_Correct+DayOfWeek, data = ABIA_cancelled, FUN = function(x) c(mn = length(x)))
colnames(DF_Q1_1)[3] <- "Cancellations"
ggplot(data = DF_Q1_1, aes(x=CSRDephr_Correct,y=as.factor(DayOfWeek))) + geom_tile(aes(fill=Cancellations))+ scale_fill_viridis(option="magma")+xlab('CSRDephr_Correct')+theme_tufte(base_family="Helvetica")+ggtitle("Cancelled flights")+xlab("Time")

```

More cancellations on weekdays, specially tuesdays 
```{r,warning=FALSE}
DF_Q2=cbind(aggregate(ArrDelay ~ as.factor(Month), data = ABIA_notcancelled, FUN = function(x) c(mn = mean(x))),aggregate(ArrDelay ~ MonthQual, data = ABIA_notcancelled, FUN = function(x) c(mn = length(x))))
DF_Q2=DF_Q2[,c(1,2,4)]
colnames(DF_Q2)[2] <- "Avg_Arr_Delay"
colnames(DF_Q2)[3] <- "flights"
require(ggplot2)
ggplot(data = ABIA_notcancelled, aes(x=as.factor(Month), y=ArrDelay)) + geom_boxplot()+scale_y_continuous(limits=c(0,200), breaks=seq(0,200,20), expand = c(0, 0))

```
There are many outliers, more spread and the median arrival delays are almost same across all the months.


```{r ,warning=FALSE }
MoM_Cancellations= table(ABIA_cancelled$CancellationCode1,ABIA_cancelled$Month)
barplot(MoM_Cancellations, main="Cancelled flights",
        xlab="Month", col=c("orange","red","blue"),
        beside = TRUE,las=2,legend = rownames(MoM_Cancellations),args.legend = list(x = "topright",bty="n"))

```

Majority of the flights cancelled during September month were because of bad weather. 
```{r,warning=FALSE}
Fromaustin=ABIA[ABIA$Origin=='AUS',]
Fromaustin$Type='Successful'
Fromaustin$Type[Fromaustin$Diverted == 1] = 'Diverted'
Fromaustin$Type[Fromaustin$Cancelled == 1] = 'Cancelled'

From_austin_successful=Fromaustin[Fromaustin$Type=='Successful',]
From_austin_successful=table(From_austin_successful$Type,From_austin_successful$Dest)
barplot(From_austin_successful, main="Successful flights",
        xlab="Destination", col=c("darkblue"),legend = rownames(From_austin_successful),beside = TRUE,srt=90,las=2)
```

More traffic towards DAL,DFW,IAH,ORD, PHX,ATL

```{r,warning=FALSE}
From_austin_failed=Fromaustin[Fromaustin$Type!='Successful',]
From_austin_failed=table(From_austin_failed$Type,From_austin_failed$Dest)
barplot(From_austin_failed, main="Cancelled/Diverted flights",
        xlab="Destination", col=c("orange","red"),
        legend = rownames(From_austin_failed),beside = TRUE,srt=90,las=2)
```

DAL,DFW,ORD have higher cancellations
More cancelled flights than diverted flights

```{r,warning=FALSE}
Fromaustin_cancelledsplit=Fromaustin[Fromaustin$Cancelled==1,]
Fromaustin_cancelledsplit= table(Fromaustin_cancelledsplit$CancellationCode1,Fromaustin_cancelledsplit$Dest)
barplot(Fromaustin_cancelledsplit, main="Cancelled flights",
        xlab="Destination", col=c("orange","red","blue"),
        beside = TRUE,las=2,legend = rownames(Fromaustin_cancelledsplit),args.legend = list(x = "topright"))

```

Most of the cancellations are due to Carrier or Weather.
ORD and DFW have very high cancelled flights because of NAS, delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc

```{r,warning=FALSE}
DF_Q4_1=aggregate(Year ~ name+Month, data = ABIA_cancelled[ABIA_cancelled$Origin=='AUS',], FUN = function(x) c(mn = length(x)))
colnames(DF_Q4_1)[3] <- "Monthly_Cancellations"
ggplot(data = DF_Q4_1, aes(x=as.factor(Month),y=name)) + geom_tile(aes(fill=Monthly_Cancellations))+ scale_fill_viridis(option="magma")+xlab('CSRDephr_Correct')+theme_tufte(base_family="Helvetica")+ggtitle("Cancelled flights")+xlab("Month")

```

DL and DFW airorts had many cancellations in the first half year, the numbers improved great deal in the later part of the year 

You can see that William P Hobby aiport and George Bush International airpoprts had more cancellations in September that increased the overall cancellations due to bad weather for the month. This is because of Hurrican Ike, tropical cyclone , sixth costliest Atlantic hurraine that severely affected Texas 







##Problem2

## Author Attribution

In the C50train directory, you have ~50 articles from each of 50 different authors (one author per directory). Use this training data (and this data alone) to build the two models. Then apply your model to the articles by the same authors in the C50test directory, which is about the same size as the training set. How well do your models do at predicting the author identities in this out-of-sample setting? Are there any sets of authors whose articles seem difficult to distinguish from one another? Which model do you prefer?


#### Loading necessary libraries

```{r echo = T, results = 'hide', warnings=FALSE}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
```

#### Reading Train and Test data from the files
```{r echo = T, results = 'hide'}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') 
}

train_list = Sys.glob('../data/ReutersC50/C50train/*/*.txt')
train = lapply(train_list, readerPlain) 



test_list = Sys.glob('../data/ReutersC50/C50test/*/*.txt')
test = lapply(test_list, readerPlain) 

mynames = train_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist


mynamestest = test_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

names(train) = mynames
names(test) = mynamestest

```

#### Extracting the Author names for Train and Test data
```{r echo = T, results = 'hide'}
authors = train_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., function(x) x[5]) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

authors_test = test_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., function(x) x[5]) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

```

#### Generating the Corpus for train and test data
```{r echo = T, results = 'hide'}
documents_raw = Corpus(VectorSource(train))
documents_raw_test = Corpus(VectorSource(test))
```

#### Tokenization

```{r echo = T, results = 'hide',warning=FALSE}
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

my_documents_test = documents_raw_test
my_documents_test = tm_map(my_documents_test, content_transformer(tolower)) # make everything lowercase
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers)) # remove numbers
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation)) # remove punctuation
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace)) ## remove excess white-space
```

#### Removing Stopwords

```{r echo = T, results = 'hide',warning=FALSE}
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))

```

#### Creating Doc-term-matrix
Here we have created 2 set of train and test datasets
The first set will involve dropping the uncommon words between the train and test dataset
The second will involve including to test matrix the terms not present in test but in train as a dummy column with 0 assigned as the value

```{r echo = T, results = 'hide',warning=FALSE}
DTM_train = DocumentTermMatrix(my_documents)
DTM_test = DocumentTermMatrix(my_documents_test)

#Removing Sparse terms 
DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_test = removeSparseTerms(DTM_test, 0.95)

#Not removing Sparse terms in case of the second train-test set
DTM_train2 = DTM_train
DTM_test2 <- DocumentTermMatrix(my_documents_test, control = list(dictionary=Terms(DTM_train2)))
```

#### Constructing TF-IDF weights for both the above sets discussed
```{r echo = T, results = 'hide',warning=FALSE}
tfidf_train = weightTfIdf(DTM_train)
tfidf_test = weightTfIdf(DTM_test)

tfidf_test2 = weightTfIdf(DTM_test2)
tfidf_train2 = weightTfIdf(DTM_train2)

```

#### Dimensionlity Reduction: Principal Component Analysis
```{r echo = T, results = 'hide',warning=FALSE}

X = as.matrix(tfidf_train)
X_test = as.matrix(tfidf_test)

X2 = as.matrix(tfidf_train2)
X_test2 = as.matrix(tfidf_test2)

#Removing columns with entries with 0 values
scrub_cols = which(colSums(X) == 0)
scrub_cols_test = which(colSums(X_test) == 0)

scrub_cols2 = which(colSums(X2) == 0)
scrub_cols_test2 = which(colSums(X_test2) == 0)

X = X[,-scrub_cols]
X_test = X_test[,-scrub_cols_test]

X2 = X2[,-scrub_cols]
X_test2 = X_test2[,-scrub_cols_test2]
```

##### For the first set dropping the uncommon words
```{r echo = T, results = 'hide',warning=FALSE}
X_test <- X_test[,intersect(colnames(X_test),colnames(X))]
X <- X[,intersect(colnames(X_test),colnames(X))]
```

##### Running PCA for the train sets and predicting the PCs for the test set 
```{r echo = T, results = 'hide',warning=FALSE}
pca_train = prcomp(X, scale=TRUE)
pca_test=predict(pca_train,newdata = X_test )

pca_train2 = prcomp(X2,scale=TRUE)
pca_test2=predict(pca_train2,newdata = X_test2 )
```

Choosing number of PCs to be selected

```{r}
plot(pca_train,type='line')
#summary(pca_train)

vars <- apply(pca_train$x, 2, var)  
props <- vars / sum(vars)
cumsum(props)
```

```{r}
plot(pca_train2,type='line')
#summary(pca_train2)

vars2 <- apply(pca_train2$x, 2, var)  
props2 <- vars2 / sum(vars2)
cumsum(props2)
```

Choosing 60% varability hence taking 207 PCs for first set and 217 PCs for second set

Attaching target variable 'author' to the train and test datsets generated after generating PCs

```{r echo = T,warning=FALSE}
df_train = data.frame(pca_train$x[,1:207])
df_train['author']=authors
df_test = data.frame(pca_test[,1:207])
df_test2 = df_test
df_test2['author']=authors_test


df_train_new = data.frame(pca_train2$x[,1:217])
df_train_new['author']=authors
df_test_new = data.frame(pca_test2[,1:217])
df_test_new2 = df_test_new
df_test_new2['author']=authors_test
```

#### Naive Bayes for intersected train-test
```{r echo = T}
library('e1071')
nb_model = naiveBayes(as.factor(author) ~., data=df_train)
nb_predictions_test = predict(nb_model,df_test)
#table(nb_predictions_test,df_test$author)
cm <- caret::confusionMatrix(nb_predictions_test,as.factor(df_test2$author))
accuracy <- cm$overall['Accuracy']
accuracy
#nb_predictions_test
#45%
```
#### Random Forest for intersected train-test

```{r echo = T, results = 'hide',warning=FALSE}

library('randomForest')
rf_model = randomForest(as.factor(author) ~ ., data=df_train, ntree=100, mtry=15, importance=TRUE)
rf_predictions_test = predict(rf_model,df_test)
#table(rf_predictions_test,df_test2$author)
cm <- caret::confusionMatrix(rf_predictions_test,as.factor(df_test2$author))
accuracy <- cm$overall['Accuracy']
accuracy
#45%
```

#### Naive Bayes for psuedo word handled train-test
```{r echo = T, warning=FALSE}
nb_model_new = naiveBayes(as.factor(author) ~., data=df_train_new)
nb_predictions_test_new = predict(nb_model_new,df_test_new)
#table(nb_predictions_test,df_test$author)
cm <- caret::confusionMatrix(nb_predictions_test_new,as.factor(df_test_new2$author))
accuracy <- cm$overall['Accuracy']
accuracy
#nb_predictions_test
#46%
```

#### Random Forest for psuedo word handled train-test
```{r echo = T,warning=FALSE}
rf_model_new = randomForest(as.factor(author) ~ ., data=df_train_new, ntree=100, mtry=15, importance=TRUE)
rf_predictions_test_new = predict(rf_model_new,df_test_new)
cm <- caret::confusionMatrix(rf_predictions_test_new,as.factor(df_test_new2$author))
accuracy <- cm$overall['Accuracy']
accuracy
#45% accuracy
```
Hence, we can say that pseudo word handled data with Naive Bayes gives us the best results with 46% accuracy.













##Problem3

```{r, include =FALSE}
library(tidyverse)
library(arules)
library(arulesViz)
```

We have multiple baskets of grocery items and will find some interesting association rules using the items in these baskets. This is how the data looks like after reading from the text file:

```{r}
groceries = scan("../data/groceries.txt", what="", sep="\n")
print(paste("number of baskets = ", length(groceries)))
head(groceries)
```

Transforming the data to make it readable in a transaction format accepted by arules package:

```{r}
y = strsplit(groceries, ",")
gro_trans = as(y, "transactions")
summary(gro_trans)
```

Few points to note from the summary:  
1. Presenece of 169 different items in all the transactions combined.  
2. Whole milk, other vegetables, rolls/buns are the most frequently bought items.    
3. 2159 out of 9835 transactions have only one item in the cart.  
4. 75% transactions have less than 6 items in the cart. 
  
Now we will run the apriori algorithm to get the rules from these transactions. In order to select threshold for support, let us consider itemsets which appear atleast in 50 baskets out of 9835. That converts to a support threshold of $(50/9835)$ = `r round(50/9835,3)`. Threshold for confidence will be fixed at 0.2 to ensure that atleast 1 of 5 baskets that contain X also have Y in them(for the rule X -> Y). Also we put in the paramenter minlen = 2 to avoid rules with NULL value of X.

```{r}
groceryrules = apriori(gro_trans, 
                     parameter=list(support=0.005, confidence=.2, minlen=2))
```

This created 872 rules. Let us plot these rules.
```{r, echo=FALSE, message=FALSE}
plot(groceryrules)
```

It is observed that rules with high lift have low support. 


Looking at the two-key plot for studying the variations in rules with size of itemset:  
```{r, echo=FALSE, message=FALSE}
plot(groceryrules, method='two-key plot')
```

All the rules of higher order have low support and itemsets of lower order have low confidence. Lets examine these rules.  

####Rules with high lift  
```{r}
inspect(subset(groceryrules, lift > 3.5))
```

####Rules with low confidence and high support rules.
```{r}
inspect(subset(groceryrules, support > 0.025 & confidence < 0.35))
```

Items in these rules are the most frequently bought items as we saw from the summary of transactions. This explains the high support of these rules and low confidence.

####Rules with high confidence and high support:  
```{r}
inspect(subset(groceryrules, support > 0.025 & confidence > 0.35))
```

####Rules on network graph
```{r}
sub1 = subset(groceryrules, subset=support > 0.025 & confidence > 0.35)
plot(sub1, method='graph')
plot(head(sub1, 5, by='lift'), method='graph')
```